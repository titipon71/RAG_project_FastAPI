import os
import re
import logging
from typing import List, Optional, Dict, Any, Union
from dataclasses import dataclass

from dotenv import load_dotenv

# --- LlamaIndex Core ---
from llama_index.core import (
    SimpleDirectoryReader,
    VectorStoreIndex,
    StorageContext,
    Document,
    Settings as LlamaSettings
)
from llama_index.core.node_parser import SemanticSplitterNodeParser
from llama_index.core.vector_stores import MetadataFilters, ExactMatchFilter
from llama_index.core.schema import NodeWithScore

# --- Integrations ---
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.llms.ollama import Ollama
from llama_index.vector_stores.chroma import ChromaVectorStore
import chromadb
from chromadb.config import Settings as ChromaSettings

# --- Logging Setup ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger("RAG_ENGINE")

load_dotenv()

# ==========================================
# 1. Configuration
# ==========================================
@dataclass
class AppConfig:
    # Paths
    DATA_DIR: str = os.getenv("RAG_DATA_DIR", "uploads")
    CHROMA_DIR: str = os.getenv("CHROMA_DIR", "./chroma_db")
    COLLECTION_NAME: str = "quickstart2"
    
    # Models
    EMBED_MODEL_NAME: str = os.getenv("EMBED_MODEL", "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
    OLLAMA_BASE_URL: str = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
    OLLAMA_MODEL: str = os.getenv("OLLAMA_MODEL", "scb10x/llama3.2-typhoon2-1b-instruct:latest")
    
    # Parameters
    CONTEXT_WINDOW: int = 4096
    NUM_OUTPUT: int = 512
    TOP_K: int = 3
    CHUNK_SIZE: int = 512 # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Splitting
    
    # Prompts
    SAFETY_SYSTEM_PROMPT: str = (
        "‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏≠‡∏±‡∏à‡∏â‡∏£‡∏¥‡∏¢‡∏∞‡∏ó‡∏µ‡πà‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏∏‡∏†‡∏≤‡∏û‡πÅ‡∏•‡∏∞‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô "
        "‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏≠‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡∏´‡πâ‡∏≤‡∏°‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏¥‡∏î "
        "‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏™‡πà‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏ô‡πÅ‡∏ó‡πá‡∏Å <think>...</think> "
        "‡∏≠‡∏¢‡πà‡∏≤‡∏Ñ‡∏¥‡∏î‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏î‡∏±‡∏á‡∏´‡∏£‡∏∑‡∏≠‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏¥‡∏î‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì."
        "‡πÄ‡∏≠‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤ '‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ ‡∏â‡∏±‡∏ô‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà üòî' "
    )

config = AppConfig()

# ==========================================
# 2. RAG Service Class (Logic ‡∏´‡∏•‡∏±‡∏Å)
# ==========================================
class RAGService:
    def __init__(self):
        logger.info("üöÄ Initializing RAG Service...")
        self._ensure_directories()
        
        # 1. Init Models (Heavy Load)
        logger.info(f"Loading Embedding Model: {config.EMBED_MODEL_NAME}")
        self.embed_model = self._init_embed_model()
        
        logger.info("Initializing Node Parser...")
        self.node_parser = self._init_node_parser()
        
        logger.info(f"Connecting to Ollama: {config.OLLAMA_MODEL}")
        self.llm = self._init_llm()
        
        # 2. Init Database & Storage
        logger.info("Connecting to ChromaDB...")
        self.chroma_collection = self._init_chroma()
        self.vector_store = ChromaVectorStore(chroma_collection=self.chroma_collection)
        self.storage_context = StorageContext.from_defaults(vector_store=self.vector_store)
        
        # 3. Load Index
        logger.info("Loading/Creating Vector Index...")
        self.index = self._load_or_create_index()
        logger.info("‚úÖ RAG Service Ready!")

    def _ensure_directories(self):
        os.makedirs(config.DATA_DIR, exist_ok=True)
        os.makedirs(config.CHROMA_DIR, exist_ok=True)

    def _init_embed_model(self) -> HuggingFaceEmbedding:
        return HuggingFaceEmbedding(
            model_name=config.EMBED_MODEL_NAME,
            device="cuda", # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô "cpu" ‡πÑ‡∏î‡πâ‡∏ñ‡πâ‡∏≤‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πå‡∏î‡∏à‡∏≠
            trust_remote_code=True
        )

    def _init_node_parser(self) -> SemanticSplitterNodeParser:
        return SemanticSplitterNodeParser.from_defaults(
            embed_model=self.embed_model,
            breakpoint_percentile_threshold=95,
            buffer_size=3
        )

    def _init_llm(self) -> Ollama:
        return Ollama(
            model=config.OLLAMA_MODEL,
            base_url=config.OLLAMA_BASE_URL,
            request_timeout=60.0,
            system_prompt=config.SAFETY_SYSTEM_PROMPT,
            context_window=config.CONTEXT_WINDOW,
            num_output=config.NUM_OUTPUT,
            additional_kwargs={
                "options": {
                    "temperature": 0.3,
                    "top_p": 0.8,
                }
            },
        )

    def _init_chroma(self):
        client = chromadb.PersistentClient(
            path=config.CHROMA_DIR,
            settings=ChromaSettings(anonymized_telemetry=False),
        )
        return client.get_or_create_collection(config.COLLECTION_NAME)

    def _load_or_create_index(self) -> VectorStoreIndex:
        try:
            # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÇ‡∏´‡∏•‡∏î‡∏à‡∏≤‡∏Å DB ‡πÄ‡∏î‡∏¥‡∏°‡∏Å‡πà‡∏≠‡∏ô (‡πÄ‡∏£‡πá‡∏ß)
            return VectorStoreIndex.from_vector_store(
                self.vector_store,
                embed_model=self.embed_model,
            )
        except Exception as e:
            logger.warning(f"Could not load index from vector store: {e}")
            logger.info("Creating new index from documents in DATA_DIR...")
            
            documents = []
            try:
                documents = SimpleDirectoryReader(config.DATA_DIR).load_data()
                for d in documents:
                    d.metadata = d.metadata or {}
                    d.metadata.setdefault("channel_id", "global")
            except ValueError:
                logger.warning("No documents found to initialize.")

            nodes = self.node_parser.get_nodes_from_documents(documents) if documents else []
            
            return VectorStoreIndex(
                nodes,
                storage_context=self.storage_context,
                embed_model=self.embed_model
            )

    def _strip_think(self, text: str) -> str:
        """‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà AI ‡∏Ñ‡∏¥‡∏î (Chain of Thought) ‡∏≠‡∏≠‡∏Å"""
        if not text:
            return text
        text = re.sub(r"<think>.*?</think>", "", text, flags=re.DOTALL | re.IGNORECASE)
        text = re.sub(r"(?i)(^|\n)\s*(analysis:|reasoning:|thoughts?:).*?(?=\n\n|\Z)", "", text, flags=re.DOTALL)
        return text.strip()

    # --- Public Methods ---

    def add_documents(self, docs: List[Document]):
        """‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÉ‡∏´‡∏°‡πà‡∏•‡∏á‡πÉ‡∏ô Index ‡πÄ‡∏î‡∏¥‡∏°"""
        if not docs:
            return
            
        logger.info(f"Adding {len(docs)} documents...")
        for d in docs:
            d.metadata = d.metadata or {}
        
        # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô Nodes
        nodes = self.node_parser.get_nodes_from_documents(docs)
        
        # Insert ‡∏•‡∏á Index (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ insert_nodes ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á VectorStoreIndex ‡πÉ‡∏´‡∏°‡πà)
        self.index.insert_nodes(nodes)
        
        # Persist ‡∏•‡∏á Storage (‡∏õ‡∏Å‡∏ï‡∏¥ ChromaVectorStore ‡∏à‡∏∞ Auto-persist ‡πÅ‡∏ï‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÑ‡∏ß‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏±‡∏ß‡∏£‡πå‡πÉ‡∏ô‡∏ö‡∏≤‡∏á version)
        self.index.storage_context.persist()
        logger.info(f"Successfully added {len(nodes)} nodes to index.")

    def delete_documents_by_metadata(self, metadata: Dict[str, Any]):
        where = {k: str(v) for k, v in metadata.items()}
        logger.info(f"Deleting documents where: {where}")
        self.chroma_collection.delete(where=where)

    def delete_documents_by_file_id(self, files_id: Union[str, int]):
        self.delete_documents_by_metadata({"files_id": str(files_id)})

    def query(self, question: str, channel_id: Union[str, int], sessions_id: Optional[int] = None) -> str:
        logger.info(f"Querying: {question} (Channel: {channel_id})")
        
        filters = MetadataFilters(
            filters=[ExactMatchFilter(key="channel_id", value=str(channel_id))]
        )
        
        query_engine = self.index.as_query_engine(
            similarity_top_k=config.TOP_K,
            filters=filters,
            llm=self.llm,                
            response_mode="compact"
        )

        response = query_engine.query(question)

        if not response.source_nodes:
            logger.info("No source nodes found.")
            return "‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏ï‡∏≠‡∏ö‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢‡∏ô‡∏∞ü§î ‡∏£‡∏ö‡∏Å‡∏ß‡∏ô‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏Å‡πà‡∏≠‡∏ô‡∏ô‡∏∞‡∏Ñ‡∏∞üòä"


        file_names = {
            node.node.metadata.get("filename") 
            for node in response.source_nodes 
            if node.node.metadata.get("filename")
        }
        if file_names:
            logger.info(f"Sources used: {file_names}")

        return self._strip_think(str(response))

    def debug_list_docs(self, channel_id: Union[str, int]):
        res = self.chroma_collection.get(where={"channel_id": str(channel_id)})
        print(f"--- DEBUG: Docs for channel {channel_id} ---")
        print(f"IDs: {res.get('ids')}")
        print(f"Metadatas: {res.get('metadatas')}")
        print("------------------------------------------")


# ==========================================
# 3. Global Instance (Eager Loading)
# ==========================================
# ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏ó‡∏≥‡πÉ‡∏´‡πâ Model ‡πÇ‡∏´‡∏•‡∏î‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡∏ó‡∏µ‡πà import ‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ
# ‡∏Ç‡πâ‡∏≠‡∏î‡∏µ: ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÅ‡∏£‡∏Å‡πÄ‡∏£‡πá‡∏ß
# ‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏µ‡∏¢: ‡∏Å‡∏¥‡∏ô Resource ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ, start server ‡∏ä‡πâ‡∏≤‡∏ô‡∏¥‡∏î‡∏ô‡∏∂‡∏á

try:
    rag_engine = RAGService()
except Exception as e:
    logger.error(f"Failed to initialize RAG Engine: {e}")
    rag_engine = None

# ==========================================
# 4. Helper Functions (Backward Compatibility)
# ==========================================
# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏û‡∏ß‡∏Å‡∏ô‡∏µ‡πâ‡∏°‡∏µ‡πÑ‡∏ß‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÇ‡∏Ñ‡πâ‡∏î‡πÄ‡∏Å‡πà‡∏≤‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏Å‡πâ‡πÄ‡∏¢‡∏≠‡∏∞
# ‡πÅ‡∏ï‡πà‡∏à‡∏£‡∏¥‡∏á‡πÜ ‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡∏ß‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å rag_engine.query() ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á‡∏à‡∏∞‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤

def rag_query_with_channel(question: str, channel_id: int, sessions_id: int) -> str:
    if rag_engine:
        return rag_engine.query(question, channel_id, sessions_id)
    return "System Error: RAG Engine not initialized."

def add_documents(docs):
    if rag_engine:
        rag_engine.add_documents(docs)

def delete_documents_by_file_id(files_id):
    if rag_engine:
        rag_engine.delete_documents_by_file_id(files_id)

def debug_list_docs_by_channel(channel_id):
    if rag_engine:
        rag_engine.debug_list_docs(channel_id)