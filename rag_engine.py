# rag_engine.py
import os
import re
from dotenv import load_dotenv

from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, StorageContext
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.llms.ollama import Ollama
from llama_index.core.vector_stores import MetadataFilters, ExactMatchFilter
from llama_index.core.node_parser import SemanticSplitterNodeParser


import chromadb
from llama_index.vector_stores.chroma import ChromaVectorStore
from chromadb.config import Settings

load_dotenv()

DATA_DIR = os.getenv("RAG_DATA_DIR", "uploads")
CHROMA_DIR = os.getenv("CHROMA_DIR", "./chroma_db")
EMBED_MODEL = os.getenv("EMBED_MODEL", "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "scb10x/llama3.2-typhoon2-1b-instruct:latest")

os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(CHROMA_DIR, exist_ok=True)




# 1) ‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£
try:
    documents = SimpleDirectoryReader(DATA_DIR).load_data()
except ValueError:
    documents = []


for d in documents:
    d.metadata = d.metadata or {}
    d.metadata.setdefault("channel_id", "global")

# 2) embedder
embed_model = HuggingFaceEmbedding(
    model_name=EMBED_MODEL,
    device="cuda",        
    trust_remote_code=True
)

# 2.1) Semantic splitter (chunk ‡πÅ‡∏ö‡∏ö semantic)
node_parser = SemanticSplitterNodeParser.from_defaults(
    embed_model=embed_model,
    breakpoint_percentile_threshold=95, 
    # ‡∏´‡∏£‡∏∑‡∏≠‡∏ñ‡πâ‡∏≤‡∏≠‡∏¢‡∏≤‡∏Å fix ‡∏Ç‡∏ô‡∏≤‡∏î chunk ‡∏Å‡πá‡πÉ‡∏ä‡πâ chunk_size=512 ‡πÅ‡∏ó‡∏ô‡πÑ‡∏î‡πâ
)

SAFETY_SYSTEM_PROMPT = (
    "‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏≠‡∏±‡∏à‡∏â‡∏£‡∏¥‡∏¢‡∏∞‡∏ó‡∏µ‡πà‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏∏‡∏†‡∏≤‡∏û‡πÅ‡∏•‡∏∞‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô "
    "‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏≠‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡∏´‡πâ‡∏≤‡∏°‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏¥‡∏î "
    "‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏™‡πà‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏ô‡πÅ‡∏ó‡πá‡∏Å <think>...</think> "
    "‡∏≠‡∏¢‡πà‡∏≤‡∏Ñ‡∏¥‡∏î‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏î‡∏±‡∏á‡∏´‡∏£‡∏∑‡∏≠‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏¥‡∏î‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì."
    "‡πÄ‡∏≠‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏°‡∏µ ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤ '‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ ‡∏â‡∏±‡∏ô‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà üòî' "
)

# 3) LLM = Ollama local
llm = Ollama(
    model=OLLAMA_MODEL,
    base_url=OLLAMA_BASE_URL,
    request_timeout=60.0,
    system_prompt=SAFETY_SYSTEM_PROMPT,
    context_window=8192,
    num_output=1536,
    additional_kwargs={
            "options": {   
                "temperature": 0.3,
                "top_p": 0.8,
            }
        },
)

# 4) Chroma persistent
chroma_client = chromadb.PersistentClient(
    path=CHROMA_DIR,
    settings=Settings(anonymized_telemetry=False),
)

COLLECTION = "quickstart2"
try:
    chroma_collection = chroma_client.get_collection(COLLECTION)
except:
    chroma_collection = chroma_client.create_collection(COLLECTION)

vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
storage_context = StorageContext.from_defaults(vector_store=vector_store)

# 5) ‡∏™‡∏£‡πâ‡∏≤‡∏á index ‡∏î‡πâ‡∏ß‡∏¢ semantic chunks
if documents:
    nodes = node_parser.get_nodes_from_documents(documents)
else:
    nodes = []

index = VectorStoreIndex.from_documents(
    nodes,
    embed_model=embed_model,
    storage_context=storage_context,
)

# 6) query engine
query_engine = index.as_query_engine(llm=llm)


def _strip_think(text: str) -> str:
    if not text:
        return text
    text = re.sub(r"<think>.*?</think>", "", text, flags=re.DOTALL | re.IGNORECASE)
    text = re.sub(r"(?i)(^|\n)\s*(analysis:|reasoning:|thoughts?:).*?(?=\n\n|\Z)", "", text, flags=re.DOTALL)
    return text.strip()

def add_documents(docs):
    for d in docs:
        d.metadata = d.metadata or {}
    # ‡πÅ‡∏õ‡∏•‡∏á documents ‡πÄ‡∏õ‡πá‡∏ô semantic chunks ‡∏Å‡πà‡∏≠‡∏ô
    nodes = node_parser.get_nodes_from_documents(docs)

    # ‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ: ‡∏™‡∏£‡πâ‡∏≤‡∏á Index ‡πÉ‡∏´‡∏°‡πà‡∏à‡∏≤‡∏Å nodes ‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ storage_context ‡πÄ‡∏î‡∏¥‡∏°
    # LlamaIndex ‡∏à‡∏∞‡∏ù‡∏±‡∏á (embed) ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏•‡∏á‡∏Ñ‡∏≠‡∏•‡πÄ‡∏•‡∏Å‡∏ä‡∏±‡∏ô‡πÄ‡∏î‡∏¥‡∏°‡πÉ‡∏´‡πâ
    VectorStoreIndex.from_documents(
        nodes,
        embed_model=embed_model,
        storage_context=storage_context,
    )


def delete_documents_by_metadata(metadata: dict):
    where = {k: str(v) for k, v in metadata.items()}
    chroma_collection.delete(where=where)

def delete_documents_by_file_id(files_id: int | str):
    where = {"files_id": str(files_id)}
    print("[RAG] delete by file_id where =", where)
    chroma_collection.delete(where=where)
    

def debug_list_docs_by_channel(channel_id: int):
    res = chroma_collection.get(where={"channel_id": str(channel_id)})
    print("[DEBUG] chroma docs for channel", channel_id)
    print("ids:", res.get("ids"))
    print("metadatas:", res.get("metadatas"))


def rag_query(question: str) -> str:
    resp = query_engine.query(question)
    return str(resp)

def rag_query_with_channel(question: str, channel_id: int) -> str:
    filters = MetadataFilters(
        filters=[ExactMatchFilter(key="channel_id", value=str(channel_id))]
    )
    retriever = index.as_retriever(similarity_top_k=5, filters=filters)
    nodes = retriever.retrieve(question)

    if not nodes:
        return "‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏ï‡∏≠‡∏ö‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢‡∏ô‡∏∞ü§î ‡∏£‡∏ö‡∏Å‡∏ß‡∏ô‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏Å‡πà‡∏≠‡∏ô‡∏ô‡∏∞‡∏Ñ‡∏∞üòä"

    file_names = []
    for n in nodes:
        if "filename" in n.metadata:
            file_names.append(n.metadata["filename"])

    # ‡∏ï‡∏≠‡∏ö‡∏õ‡∏ô‡πÑ‡∏õ‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏£‡∏∏‡∏õ
    from llama_index.core.response_synthesizers import get_response_synthesizer
    synth = get_response_synthesizer(llm=llm, response_mode="refine")
    resp = synth.synthesize(question, nodes)

    answer = _strip_think(str(resp))
    
    if file_names:
        print(f"[RAG] files used: {set(file_names)}")
        return f"{answer}"
    return answer